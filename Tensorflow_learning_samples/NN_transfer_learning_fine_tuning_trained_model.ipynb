{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Get helper_functions.py script from course GitHub\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "# Import helper functions we're going to use\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir\n"
      ],
      "metadata": {
        "id": "NohdbGFSA6Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Download zip file of 10_food_classes images\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
        "\n",
        "# Unzip the downloaded file\n",
        "zip_ref = zipfile.ZipFile(\"10_food_classes_all_data.zip\", \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "aUIlzL3EDZ_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_SHAPE = (224,224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dir = \"10_food_classes_all_data/train/\"\n",
        "test_dir = \"10_food_classes_all_data/test/\"\n",
        "\n",
        "\n",
        "print(\"Training Data:\")\n",
        "train_data = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
        "                                                                            image_size=IMAGE_SHAPE,\n",
        "                                                                            label_mode=\"categorical\", # what type are the labels?\n",
        "                                                                            batch_size=BATCH_SIZE) # ba\n",
        "print(\"Testing Data:\")\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
        "                                                                image_size = IMAGE_SHAPE,\n",
        "                                                                batch_size = BATCH_SIZE,\n",
        "                                                                label_mode = \"categorical\")"
      ],
      "metadata": {
        "id": "Kc4FWwyN92aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 10% of the data of the 10 classes\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "unzip_data(\"10_food_classes_10_percent.zip\")\n",
        "\n",
        "# Create training and test directories\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
        "                                                                            image_size=IMAGE_SHAPE,\n",
        "                                                                            label_mode=\"categorical\", # what type are the labels?\n",
        "                                                                            batch_size=BATCH_SIZE) # batch_size is 32 by default, this is generally a good numbe"
      ],
      "metadata": {
        "id": "rZQ2XKteQDCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "# from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# NEW: Newer versions of TensorFlow (2.10+) can use the tensorflow.keras.layers API directly for data augmentation\n",
        "data_augmentation = keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "  layers.RandomZoom(0.2),\n",
        "  layers.RandomHeight(0.2),\n",
        "  layers.RandomWidth(0.2),\n",
        "], name =\"data_augmentation\")\n"
      ],
      "metadata": {
        "id": "5BwCay6yJgKr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_base_model(input_shape: tuple[int, int, int] = (224, 224, 3),\n",
        "                      output_shape: int = 10,\n",
        "                      learning_rate: float = 0.001,\n",
        "                      training: bool = False) -> tf.keras.Model:\n",
        "    \"\"\"\n",
        "    Create a model based on EfficientNetV2B0 with built-in data augmentation.\n",
        "\n",
        "    Parameters:\n",
        "    - input_shape (tuple): Expected shape of input images. Default is (224, 224, 3).\n",
        "    - output_shape (int): Number of classes for the output layer. Default is 10.\n",
        "    - learning_rate (float): Learning rate for the Adam optimizer. Default is 0.001.\n",
        "    - training (bool): Whether the base model is trainable. Default is False.\n",
        "\n",
        "    Returns:\n",
        "    - tf.keras.Model: The compiled model with specified input and output settings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create base model\n",
        "    base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
        "    base_model.trainable = training\n",
        "\n",
        "    # Setup model input and outputs with data augmentation built-in\n",
        "    inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "    x = data_augmentation(inputs)\n",
        "    x = base_model(x, training=False)  # pass augmented images to base model but keep it in inference mode\n",
        "    x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "    outputs = layers.Dense(units=output_shape, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=\"categorical_crossentropy\",\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create an instance of model_2 with our new function\n",
        "model_0 = create_base_model()"
      ],
      "metadata": {
        "id": "rvb7IhA-VcK-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup checkpoint path\n",
        "checkpoint_path = \"_model_checkpoints_weights/checkpoint.ckpt\" # note: remember saving directly to Colab is temporary\n",
        "\n",
        "# Create a ModelCheckpoint callback that saves the model's weights only\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                         save_weights_only=True, # set to False to save the entire model\n",
        "                                                         save_best_only=True, # save only the best model weights instead of a model every epoch\n",
        "                                                         save_freq=\"epoch\", # save every epoch\n",
        "                                                         verbose=1)"
      ],
      "metadata": {
        "id": "sL1ATIaxUn2S"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_0 = model_0.fit(train_data_10_percent,\n",
        "                        epochs = 5,\n",
        "                        steps_per_epoch = len(train_data_10_percent),\n",
        "                        validation_data = test_data,\n",
        "                        validation_steps = len(test_data),\n",
        "                        callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "aD4bxrNGA3kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine tunning the model and training more layers"
      ],
      "metadata": {
        "id": "bdQPZpcZH36Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a new model to play around with\n",
        "model_1 = create_base_model(learning_rate = 0.0001)\n",
        "\n",
        "# Load previously checkpointed weights\n",
        "model_1.load_weights(checkpoint_path)\n",
        "\n",
        "# Unfreeze top 10 layers of the efficentnetv2\n",
        "model_1.layers[2].trainable = True\n",
        "for layer in model_1.layers[2].layers[:-10]:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "i95aKpixNPZG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which layers are tuneable in the base model\n",
        "for layer_number, layer in enumerate(model_1.layers[2].layers):\n",
        "  print(layer_number, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "LhCMRzCxaAnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile the model\n",
        "model_1.compile(loss = \"categorical_crossentropy\",\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "                metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "rPSBqDcBaEQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_1 = model_1.fit(train_data_10_percent,\n",
        "                        epochs = 10,\n",
        "                        validation_data=test_data,\n",
        "                        initial_epoch = 5,\n",
        "                        validation_steps=int(0.25 * len(test_data)))\n"
      ],
      "metadata": {
        "id": "J2dTGvXuOEpG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}